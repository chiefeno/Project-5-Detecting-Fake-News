{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting words to use as features\n",
    "In this notebook, we'll be using TFIDF to narrow down the tens of thousands of unique words in our articles and their titles into a more condensed list of important words. \n",
    "\n",
    "### I. [Initial vectorization](#Initial-vectorization)\n",
    "### II. [Stemming](#Stemming)\n",
    "### III. [TFIDF vectorization](#TFIDF-vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Git would not allow us to push a CSV containing the entire dataset so we had to split it up by X and y as well as by train and test sets. Below, we read in all of those files and put them back into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../datasets/X_train_w_SA.csv')\n",
    "X_test = pd.read_csv('../datasets/X_test_w_SA.csv')\n",
    "y_train = pd.read_csv('../datasets/y_train.csv')\n",
    "y_test = pd.read_csv('../datasets/y_test.csv')\n",
    "\n",
    "y_train['train_dataset'] = 1\n",
    "y_test['train_dataset'] = 0\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = pd.concat([X, y], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll vectorize the combined title and text to get a sense of the frequency with which each word is used in these articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating titles and text\n",
    "df['all_text'] = [np.nan]*df.shape[0]\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    text = df.loc[i, 'text']\n",
    "    title = df.loc[i, 'title']\n",
    "    all_text = title + ' ' + text\n",
    "    df.loc[i,'all_text'] = all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing into a large dataframe\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(df['text'])\n",
    "vec_df = pd.DataFrame(cvec.transform(df['text']).todense(),\n",
    "                      columns = cvec.get_feature_names())\n",
    "\n",
    "print(vec_df.shape)\n",
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizing this tool to narrow to actual dictionary words\n",
    "# http://openbookproject.net/courses/python4fun/spellcheck.html\n",
    "\n",
    "words = open(\"../datasets/spellcheck.txt\").readlines()\n",
    "words = [word.strip() for word in words]\n",
    "\n",
    "real_words = []\n",
    "\n",
    "for i, col in enumerate(vec_df):\n",
    "    if col in words:\n",
    "        real_words.append(col)\n",
    "    if i % 20000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "vec_df = vec_df[real_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>0</th>\n",
       "      <th>aal</th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abandons</th>\n",
       "      <th>...</th>\n",
       "      <th>zonation</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zu</th>\n",
       "      <th>zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39853</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39854</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39855</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39858 rows × 28602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0      aal  aardvark  aba  aback  abacus  abandon  abandoned  abandoning  \\\n",
       "0        0         0    0      0       0        0          0           0   \n",
       "1        0         0    0      0       0        0          0           0   \n",
       "2        0         0    0      0       0        0          0           0   \n",
       "3        0         0    0      0       0        0          0           0   \n",
       "4        0         0    0      0       0        0          0           0   \n",
       "...    ...       ...  ...    ...     ...      ...        ...         ...   \n",
       "39853    0         0    0      0       0        0          0           0   \n",
       "39854    0         0    0      0       0        0          0           0   \n",
       "39855    0         0    0      0       0        0          0           0   \n",
       "39856    0         0    0      0       0        0          0           0   \n",
       "39857    0         0    0      0       0        0          0           0   \n",
       "\n",
       "0      abandonment  abandons  ...  zonation  zone  zoned  zones  zoning  zoo  \\\n",
       "0                0         0  ...         0     0      0      0       0    0   \n",
       "1                0         0  ...         0     0      0      0       0    0   \n",
       "2                0         0  ...         0     0      0      0       0    0   \n",
       "3                0         0  ...         0     0      0      0       0    0   \n",
       "4                0         0  ...         0     0      0      0       0    0   \n",
       "...            ...       ...  ...       ...   ...    ...    ...     ...  ...   \n",
       "39853            0         0  ...         0     0      0      0       0    0   \n",
       "39854            0         0  ...         0     0      0      0       0    0   \n",
       "39855            0         0  ...         0     0      0      0       0    0   \n",
       "39856            0         0  ...         0     0      0      0       0    0   \n",
       "39857            0         0  ...         0     0      0      0       0    0   \n",
       "\n",
       "0      zoom  zorro  zu  zucchini  \n",
       "0         0      0   0         0  \n",
       "1         0      0   0         0  \n",
       "2         0      0   0         0  \n",
       "3         0      0   0         0  \n",
       "4         0      0   0         0  \n",
       "...     ...    ...  ..       ...  \n",
       "39853     0      0   0         0  \n",
       "39854     0      0   0         0  \n",
       "39855     0      0   0         0  \n",
       "39856     0      0   0         0  \n",
       "39857     0      0   0         0  \n",
       "\n",
       "[39858 rows x 28602 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "In order to find the most important words, we concatenated the titles with their corresponding text and created a new version of that text where only the stems remained. We then vectorized again to find the frequency and importance of use for each stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating stemmer and creating a list of real words\n",
    "stemmer = PorterStemmer()\n",
    "word_list = vec_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3000\n",
      "6000\n",
      "9000\n",
      "12000\n",
      "15000\n",
      "18000\n",
      "21000\n",
      "24000\n",
      "27000\n",
      "30000\n",
      "33000\n",
      "36000\n",
      "39000\n"
     ]
    }
   ],
   "source": [
    "# creating a column for the stemmed versions of the text\n",
    "df['stemmed'] = [np.nan]*df.shape[0]\n",
    "\n",
    "# concatenating stems from text into strings in the stemmed column\n",
    "for i, t in enumerate(list(df['all_text'])):\n",
    "    stemmed = []\n",
    "    for word in t.split(' '):\n",
    "        if word.lower() in word_list:\n",
    "            stem = stemmer.stem(word)\n",
    "            stemmed.append(stem)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    df.loc[i,'stemmed'] = ' '.join(stemmed)\n",
    "    if (i % 3000) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we vectorize the text again so we have a dataframe of only the stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(df['stemmed'])\n",
    "\n",
    "words_cv = cv.transform(df['stemmed'])\n",
    "\n",
    "stems_df = pd.DataFrame(words_cv.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF vectorization\n",
    "\n",
    "In order to determine the importance of words in one \"document\" relative to another (in this case, one document is fake news and the other is real news), we concatenated all of the stemmed text into two long strings, one for each class and compared those to one another. The result was a dataframe with the stems and their relative importance to each \"document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_string = ''\n",
    "f_string = ''\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    string = df.loc[i,'stemmed']\n",
    "    if df.loc[i,'is_true'] == 1:\n",
    "        r_string += ' '+string\n",
    "    else: \n",
    "        f_string += ' '+string\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tvec.fit([r_string,f_string])\n",
    "\n",
    "tv = pd.DataFrame(tvec.transform([r_string, f_string]).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['real', 'fake'])\n",
    "\n",
    "tv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then utilized this dataframe to capture the most important words from each document as our features. We set a threshold of 0.01 in order to narrow the words down and got a list of 577 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_t = tv.T\n",
    "\n",
    "r_words = set(tv_t[tv_t['fake'] > 0.01].index)\n",
    "f_words = set(tv_t[tv_t['fake'] > 0.01].index)\n",
    "selected_words = list(r_words.union(f_words))\n",
    "selected_words.sort()\n",
    "len(selected_words)a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those selected words, we narrowed dataframe and added those 577 columns into the same dataframe as our engineered features relating to punctuation, sentiment, or parts of speech. In total, we had a dataframe with our approximately 40,000 samples and 626 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df.merge(stems_df[selected_words], right_index = True, left_index = True)\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of the punctuation, sentiment, and parts of speech columns\n",
    "feats = list(full_df.columns[3:52])\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding all of our selected words for a full list of our features\n",
    "feats.extend(selected_words)\n",
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting new X_train and X_test CSVs with our chosen features\n",
    "full_df.loc[full_df['train_dataset'] == 1, feats].to_csv('../datasets/X_train_w_SA_and_words.csv', index = False)\n",
    "full_df.loc[full_df['train_dataset'] == 0, feats].to_csv('../datasets/X_test_w_SA_and_words.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to explore some other features we can add to our model before doing some EDA and actual modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
